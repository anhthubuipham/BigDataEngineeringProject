{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a241a-6665-47a1-b9b2-37a130596801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def get_latest_file(pattern):\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Keine Datei gefunden für Muster: {pattern}\")\n",
    "    return max(files, key=os.path.getctime)\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='172.29.16.101:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# GPU\n",
    "gpu_gh_df = pd.read_csv(get_latest_file(\"geizhals_gpu_*.csv\"))\n",
    "for _, row in gpu_gh_df.iterrows():\n",
    "    producer.send(\"geizhals-gpu\", value=row.to_dict())\n",
    "\n",
    "# RAM\n",
    "ram_gh_df = pd.read_csv(get_latest_file(\"geizhals_ram_*.csv\"))\n",
    "for _, row in ram_gh_df.iterrows():\n",
    "    producer.send(\"geizhals-ram\", value=row.to_dict())\n",
    "\n",
    "# SSD\n",
    "ssd_gh_df = pd.read_csv(get_latest_file(\"geizhals_ssd_*.csv\"))\n",
    "for _, row in ssd_gh_df.iterrows():\n",
    "    producer.send(\"geizhals-ssd\", value=row.to_dict())\n",
    "\n",
    "# CPU (Intel + AMD)\n",
    "amd_cpu_df = pd.read_csv(get_latest_file(\"geizhals_amd_cpus_*.csv\"))\n",
    "intel_cpu_df = pd.read_csv(get_latest_file(\"geizhals_intel_cpus_*.csv\"))\n",
    "cpu_gh_df = pd.concat([amd_cpu_df, intel_cpu_df], ignore_index=True)\n",
    "for _, row in cpu_gh_df.iterrows():\n",
    "    producer.send(\"geizhals-cpu\", value=row.to_dict())\n",
    "\n",
    "# Steam HW Summary\n",
    "steam_summary_df = pd.read_csv(get_latest_file(\"steam_hwsurvey_summary_*.csv\"))\n",
    "steam_summary_df[\"share_numeric\"] = steam_summary_df[\"share\"].str.replace(\"%\", \"\").astype(float) / 100.0\n",
    "for _, row in steam_summary_df.iterrows():\n",
    "    producer.send(\"steam-hwsurvey-summary\", value=row.to_dict())\n",
    "\n",
    "producer.flush()\n",
    "print(\"Alle Daten erfolgreich an Kafka gesendet!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab2470-eac7-41cf-84a8-82f25a88e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col, avg, regexp_replace\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Geizhals-Steam-Streaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schemata\n",
    "gpu_schema = StructType() \\\n",
    "    .add(\"Produktname\", StringType()) \\\n",
    "    .add(\"VRAM\", StringType()) \\\n",
    "    .add(\"Preis_EUR\", DoubleType())\n",
    "\n",
    "ram_schema = StructType() \\\n",
    "    .add(\"Produktname\", StringType()) \\\n",
    "    .add(\"Kapazitaet\", StringType()) \\\n",
    "    .add(\"Preis_EUR\", DoubleType()) \\\n",
    "    .add(\"RAM_Typ\", StringType()) \\\n",
    "    .add(\"CAS_Latency\", StringType())\n",
    "\n",
    "ssd_schema = StructType() \\\n",
    "    .add(\"Produktname\", StringType()) \\\n",
    "    .add(\"Speicher\", StringType()) \\\n",
    "    .add(\"Preis_EUR\", DoubleType())\n",
    "\n",
    "cpu_schema = StructType() \\\n",
    "    .add(\"Title\", StringType()) \\\n",
    "    .add(\"Price_EUR\", DoubleType()) \\\n",
    "    .add(\"Kerne\", IntegerType()) \\\n",
    "    .add(\"Turbotakt_GHz\", DoubleType()) \\\n",
    "    .add(\"Basistakt_GHz\", DoubleType())\n",
    "\n",
    "steam_schema = StructType() \\\n",
    "    .add(\"category\", StringType()) \\\n",
    "    .add(\"most_common\", StringType()) \\\n",
    "    .add(\"share\", StringType())\n",
    "\n",
    "# Helper zum Streamlesen\n",
    "def read_stream(topic, schema):\n",
    "    return spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"172.29.16.101:9092\") \\\n",
    "        .option(\"subscribe\", topic) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load() \\\n",
    "        .selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "        .select(from_json(col(\"json_str\"), schema).alias(\"data\")) \\\n",
    "        .select(\"data.*\")\n",
    "\n",
    "# Streams lesen\n",
    "gpu_df = read_stream(\"geizhals-gpu\", gpu_schema)\n",
    "ram_df = read_stream(\"geizhals-ram\", ram_schema)\n",
    "ssd_df = read_stream(\"geizhals-ssd\", ssd_schema)\n",
    "cpu_df = read_stream(\"geizhals-cpu\", cpu_schema)\n",
    "steam_df = read_stream(\"steam-hwsurvey-summary\", steam_schema) \\\n",
    "    .withColumn(\"share_float\", regexp_replace(col(\"share\"), \"%\", \"\").cast(\"float\") / 100)\n",
    "\n",
    "# WriteStreams → Parquet speichern\n",
    "gpu_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/tmp/gpu_data\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/gpu_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "ram_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/tmp/ram_data\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/ram_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "ssd_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/tmp/ssd_data\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/ssd_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "cpu_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/tmp/cpu_data\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/cpu_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "steam_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/tmp/steam_data\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/steam_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# Laufend\n",
    "spark.streams.awaitAnyTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
